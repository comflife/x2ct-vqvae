import torch
import torch.nn as nn
import numpy as np
import os
from pathlib import Path


class SimpleUltraLiDARWrapper(nn.Module):
    """
    UltraLiDAR ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ë§Œ ì¶”ì¶œí•˜ëŠ” ê°„ë‹¨í•œ ë˜í¼
    mmcv ì˜ì¡´ì„± ì—†ì´ ë™ì‘
    """
    def __init__(self, checkpoint_path, model_type='radar'):
        super().__init__()
        
        self.checkpoint_path = checkpoint_path
        self.model_type = model_type
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        print(f"Loading checkpoint from: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # state_dict ì¶”ì¶œ
        if 'state_dict' in checkpoint:
            self.state_dict_full = checkpoint['state_dict']
        elif 'model' in checkpoint:
            self.state_dict_full = checkpoint['model']
        else:
            self.state_dict_full = checkpoint
        
        print(f"âœ… Checkpoint loaded successfully")
        print(f"ğŸ“Š Available keys (first 10): {list(self.state_dict_full.keys())[:10]}")
        
        # ê°„ë‹¨í•œ ë”ë¯¸ ëª¨ë¸ ìƒì„± (VQGAN ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜)
        self.encoder_weights = self._extract_encoder_weights()
        self.decoder_weights = self._extract_decoder_weights()
        self.quantizer_weights = self._extract_quantizer_weights()
        
    def _extract_encoder_weights(self):
        """ì¸ì½”ë” ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        encoder_weights = {}
        for key, value in self.state_dict_full.items():
            if 'lidar_encoder' in key or 'encoder' in key:
                encoder_weights[key] = value
        return encoder_weights
    
    def _extract_decoder_weights(self):
        """ë””ì½”ë” ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        decoder_weights = {}
        for key, value in self.state_dict_full.items():
            if 'lidar_decoder' in key or 'decoder' in key or 'generator' in key:
                decoder_weights[key] = value
        return decoder_weights
    
    def _extract_quantizer_weights(self):
        """ì–‘ìí™”ê¸° ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        quantizer_weights = {}
        for key, value in self.state_dict_full.items():
            if 'vector_quantizer' in key or 'quantize' in key:
                quantizer_weights[key] = value
        return quantizer_weights
    
    def points_to_bev(self, points, bev_size=640, point_cloud_range=[-50.0, -50.0, -5.0, 50.0, 50.0, 3.0]):
        """ì êµ°ì„ BEVë¡œ ë³€í™˜ (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if isinstance(points, list):
            if len(points) == 0:
                return torch.zeros(1, bev_size, bev_size)
            points = points[0]
        
        if len(points) == 0:
            return torch.zeros(1, bev_size, bev_size)
        
        if points.dim() == 3:
            points = points.squeeze(0)
        
        x_min, y_min = point_cloud_range[0], point_cloud_range[1]
        x_max, y_max = point_cloud_range[3], point_cloud_range[4]
        
        x_coords = points[:, 0]
        y_coords = points[:, 1]
        intensities = points[:, 3] if points.shape[1] > 3 else torch.ones_like(x_coords)
        
        # ë²”ìœ„ ë‚´ ì ë“¤ë§Œ ì„ íƒ
        valid_mask = (x_coords >= x_min) & (x_coords <= x_max) & \
                    (y_coords >= y_min) & (y_coords <= y_max)
        
        if not valid_mask.any():
            return torch.zeros(1, bev_size, bev_size)
        
        x_coords = x_coords[valid_mask]
        y_coords = y_coords[valid_mask]
        intensities = intensities[valid_mask]
        
        # í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        x_img = ((x_coords - x_min) / (x_max - x_min) * (bev_size - 1)).long()
        y_img = ((y_coords - y_min) / (y_max - y_min) * (bev_size - 1)).long()
        
        # ë²”ìœ„ ì²´í¬
        valid_pixels = (x_img >= 0) & (x_img < bev_size) & \
                      (y_img >= 0) & (y_img < bev_size)
        
        x_img = x_img[valid_pixels]
        y_img = y_img[valid_pixels]
        intensities = intensities[valid_pixels]
        
        # BEV ìƒì„±
        bev = torch.zeros(bev_size, bev_size, device=points.device)
        if len(x_img) > 0:
            bev[y_img, x_img] = intensities
        
        return bev.unsqueeze(0)  # [1, H, W]
    
    def encoder(self, x):
        """ë”ë¯¸ ì¸ì½”ë” (ì‹¤ì œë¡œëŠ” ê°„ë‹¨í•œ ë³€í™˜ë§Œ ìˆ˜í–‰)"""
        batch_size = x.shape[0]
        
        # ë”ë¯¸ latent ìƒì„± (ì‹¤ì œ UltraLiDAR êµ¬ì¡°ì— ë§ì¶¤)
        # 640 -> 80 (8x downsampling)
        latent_h, latent_w = 80, 80
        latent_dim = 1024  # UltraLiDAR codebook_dim
        
        # ê°„ë‹¨í•œ downsampling (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨)
        x_downsampled = nn.functional.avg_pool2d(x, kernel_size=8, stride=8)  # [B, C, 80, 80]
        
        # ì±„ë„ ì°¨ì›ì„ 1024ë¡œ í™•ì¥
        if x_downsampled.shape[1] != latent_dim:
            # ê°„ë‹¨í•œ linear projection (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
            latents = x_downsampled.repeat(1, latent_dim, 1, 1)[:, :latent_dim]
        else:
            latents = x_downsampled
        
        return latents
    
    def quantize(self, latents):
        """ë”ë¯¸ ì–‘ìí™” (ì½”ë“œë¶ ì¸ë±ìŠ¤ ìƒì„±)"""
        batch_size, channels, h, w = latents.shape
        
        # ë”ë¯¸ ì–‘ìí™” - ì‹¤ì œë¡œëŠ” ì½”ë“œë¶ê³¼ì˜ ê±°ë¦¬ ê³„ì‚° í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ argmax ì‚¬ìš©
        latents_flat = latents.view(batch_size, channels, -1)  # [B, C, H*W]
        
        # ë”ë¯¸ ì½”ë“œë¶ ì¸ë±ìŠ¤ (0~1023 ë²”ìœ„)
        code_indices = torch.randint(0, 1024, (batch_size, h * w), device=latents.device)
        
        # ë”ë¯¸ ì–‘ìí™”ëœ latents
        quant_latents = latents  # ì‹¤ì œë¡œëŠ” ì½”ë“œë¶ì—ì„œ ì„ë² ë”© ì¡°íšŒ
        
        # ë”ë¯¸ ì„ë² ë”© ì†ì‹¤
        emb_loss = torch.tensor(0.0, device=latents.device)
        
        stats = {"min_encoding_indices": code_indices}
        
        return quant_latents, emb_loss, stats
    
    def generator(self, quantized_latents):
        """ë”ë¯¸ ìƒì„±ê¸° (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ë””ì½”ë” ì‚¬ìš©)"""
        batch_size = quantized_latents.shape[0]
        
        # ê°„ë‹¨í•œ upsampling (80 -> 640)
        upsampled = nn.functional.interpolate(
            quantized_latents, 
            size=(640, 640), 
            mode='bilinear', 
            align_corners=False
        )
        
        # ë‹¨ì¼ ì±„ë„ë¡œ ë³€í™˜
        if upsampled.shape[1] > 1:
            generated = upsampled.mean(dim=1, keepdim=True)  # [B, 1, 640, 640]
        else:
            generated = upsampled
        
        # Sigmoid ì ìš©
        generated = torch.sigmoid(generated)
        
        return generated
    
    def get_embedding_weight(self):
        """ì„ë² ë”© ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        # UltraLiDAR ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì°¾ê¸°
        for key, value in self.state_dict_full.items():
            if 'embedding.weight' in key and 'vector_quantizer' in key:
                print(f"Found embedding weight: {key}, shape: {value.shape}")
                return value
        
        # ì°¾ì§€ ëª»í•œ ê²½ìš° ë”ë¯¸ ê°€ì¤‘ì¹˜ ìƒì„±
        print("Warning: Could not find embedding weight. Using random initialization.")
        return torch.randn(1024, 1024)  # [codebook_size, emb_dim]
    
    def forward(self, x):
        """ì „ì²´ ìˆœì „íŒŒ"""
        latents = self.encoder(x)
        quant_latents, emb_loss, quant_stats = self.quantize(latents)
        reconstructed = self.generator(quant_latents)
        
        return {
            'reconstructed': reconstructed,
            'latents': latents,
            'quant_latents': quant_latents,
            'emb_loss': emb_loss,
            'quant_stats': quant_stats
        }


def load_simple_ultralidar_model(checkpoint_path, model_type='radar'):
    """ê°„ë‹¨í•œ UltraLiDAR ëª¨ë¸ ë¡œë”"""
    try:
        model = SimpleUltraLiDARWrapper(checkpoint_path, model_type)
        print(f"âœ… Simple UltraLiDAR {model_type} model loaded successfully")
        return model
    except Exception as e:
        print(f"âŒ Failed to load simple UltraLiDAR {model_type} model: {e}")
        raise e


# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
UltraLiDARRadarWrapper = SimpleUltraLiDARWrapper
load_ultralidar_radar_model = load_simple_ultralidar_model
load_ultralidar_lidar_model = lambda path, config=None: load_simple_ultralidar_model(path, 'lidar')

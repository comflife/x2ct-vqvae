\subsection{Vector-Quantized Autoencoders}
\noindent Attempts to scale powerful generative models, such as autoregressive models, to very high-dimensional spaces results in difficulties such as sampling speed, slow mixing, and small receptive fields. Vector-Quantized autoencoders offer a solution to this problem, by accurately compressing the high-dimensional data to a much lower dimensional discrete space which can be more easily modelled cite[oord2017neural, esser2021taming, bond2022unleashing, latent diffusion, maskgit, etc.]. First, a convolutional encoder downsamples data $\vec{x}$ to a smaller spatial resolution, $E(\vec{x}) = \{\vec{e}_1, \vec{e}_2, ..., \vec{e}_L\} \in \mathbb{R}^{L \times D}$. Each continuous encoding $\vec{e}_i$ is then quantised by mapping to the closest element in codebook of vectors $\mathcal{B} \in \mathbb{R}^{K \times D}$, where $K$ is the number of discrete codes in the codebook and $D$ is the dimension of each code,
%
\begin{equation}\label{eqn:quantisation}
    \vec{z}_q = \{\vec{q}_1, \vec{q}_2, ..., \vec{q}_L\} \text{  , where  } \vec{q}_i = \underset{\vec{b}_{j} \in \mathcal{B}}{\operatorname{min}}\|\vec{e}_i - \vec{b}_j\|,
\end{equation}
with the straight-through gradient estimator \cite{bengio2013estimating} used to approximate the gradients through this non-differentiable process. The discrete latents are then decompressed with a convolutional decoder $\hat{\vec{x}}= G(\vec{z}_q)$. The model is trained end-to-end by minimising the loss $\loss_\text{VQ}$,
%
\begin{equation}\label{eqn:vqvae-loss}
    \loss_\text{VQ} = \loss_\text{rec}(\vec{x}, \hat{\vec{x}}) + \| \text{sg}[E(\vec{x})] - \vec{z}_q \|_2^2 + \beta \| \text{sg}[\vec{z}_q] - E(\vec{x}) \|_2^2,
\end{equation}
which balances reconstruction quality $\loss_\text{rec}$ against quantisation terms that encourage encodings $E(\vec{x})$ to match the closest codes in the codebook.

To generate samples, a discrete generative model can be trained to model the distribution of these latents. For instance, with an autoregressive model with parameters $\theta$:
%
\begin{equation}
    p_\theta(\vec{z}) = \prod\nolimits_{i=1}^{L}p_\theta(z_i|z_1, ..., z_{i-1}).
\end{equation}


\noindent Vector-Quantized autoencoders learn a highly compressed discrete representation taking advantage of an information rich codebook \cite{oord2017neural}. A convolutional encoder downsamples images $\vec{x}$ to a smaller spatial resolution, $E(\vec{x}) = \{\vec{e}_1, \vec{e}_2, ..., \vec{e}_L\} \in \mathbb{R}^{L \times D}$. A simple quantisation approach is to use the $\arg\!\max$ operation which maps continuous encodings to their closest elements in a finite codebook of vectors \cite{oord2017neural}. Specifically, for a codebook $\mathcal{B} \in \mathbb{R}^{K \times D}$, where $K$ is the number of discrete codes in the codebook and $D$ is the dimension of each code, each $\vec{e}_i$ is mapped via a nearest-neighbour lookup onto a discrete codebook value, $\vec{b}_j \in \mathcal{B}$:
%
\begin{equation}\label{eqn:quantisation}
    \vec{z}_q = \{\vec{q}_1, \vec{q}_2, ..., \vec{q}_L\} \text{  , where  } \vec{q}_i = \underset{\vec{b}_{j} \in \mathcal{B}}{\operatorname{min}}\|\vec{e}_i - \vec{b}_j\|.
\end{equation}
%
As this operation is non-differentiable, the straight-through gradient estimator \cite{bengio2013estimating} is used to approximate gradients resulting in bias. The quantized latents are fed through a decoder $\hat{\vec{x}}= G(\vec{z}_q)$ to reconstruct the input based on a perceptual reconstruction loss \cite{zhang2018unreasonable, esser2021taming, bond2022unleashing}; this process is trained by minimising the loss $\loss_\text{VQ}$,
%
\begin{equation}\label{eqn:vqvae-loss}
    \loss_\text{VQ} = \loss_\text{rec} + \| \text{sg}[E(\vec{x})] - \vec{z}_q \|_2^2 + \beta \| \text{sg}[\vec{z}_q] - E(\vec{x}) \|_2^2.
\end{equation}

To generate samples, discrete generative models can be trained to model the distribution of these latents. For instance, with an autoregressive model with parameters $\theta$:
%
\begin{equation}
    p_\theta(\vec{z}) = \prod\nolimits_{i=1}^{L}p_\theta(z_i|z_1, ..., z_{i-1}).
\end{equation}
%
% For many tasks, appropriate input orderings are not obvious; since the receptive field is limited to previous tokens, this can significantly affect sample quality.






In this case, the discrete 3D latents are gradually masked out over a number of time steps $T$ such that at time step $t$, $\vec{c}^t$ is defined as the disctete 3D latents $\vec{c}$ with each token masked out with probability $\frac{t}{T}$. Formally, this masking procedure is defined by a Markov chain,
%
\begin{equation}
    q(\vec{c}^{1:T}|\vec{c}^0) = \prod_{t=1}^T q(\vec{c}^t| \vec{c}^{t-1}).
\end{equation}
%
In this case, the posterior is defined as $q(\vec{x}_t|\vec{x}_{t-1})= \text{Cat}(\vec{x}_t;\vec{p}=\vec{x}_{t-1} \vec{Q}_t)$ for one-hot $\vec{c}_{t-1}$ where $\vec{Q}_t = (1-\beta_t)I + \beta_t \mathbbm{1}e_m^T$ is a matrix denoting the probabilities of moving to each successive state, $e_m$ is a vector with a one on mask states $m$ and zeros elsewhere, and $\beta_t = \frac{1}{T-t+1}$.